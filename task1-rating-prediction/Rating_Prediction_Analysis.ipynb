{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fynd AI Intern Assessment - Task 1\n",
    "## Rating Prediction via Prompting\n",
    "\n",
    "This notebook implements and evaluates 3 different prompting approaches for classifying Yelp reviews into 1-5 star ratings using OpenAI's GPT models.\n",
    "\n",
    "### Objectives:\n",
    "1. Design 3 distinct prompting approaches\n",
    "2. Evaluate accuracy, JSON validity, and consistency\n",
    "3. Compare performance across approaches\n",
    "4. Generate comprehensive analysis report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(\"üöÄ Starting Fynd AI Assessment - Task 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Configuration\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "print(\"üîë OpenAI API configured\")\n",
    "print(\"ü§ñ Ready for real-time AI analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Loading and preparing the Yelp reviews dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample Yelp reviews dataset\n",
    "def create_sample_data(size=50):\n",
    "    \"\"\"Create sample Yelp reviews for demonstration\"\"\"\n",
    "    \n",
    "    sample_reviews = [\n",
    "        # 5-star reviews\n",
    "        {\"text\": \"Amazing food and excellent service! The staff was incredibly friendly and the atmosphere was perfect. Will definitely come back!\", \"stars\": 5},\n",
    "        {\"text\": \"Outstanding restaurant! Every dish was perfectly prepared and the service was impeccable. Best dining experience I've had in years!\", \"stars\": 5},\n",
    "        {\"text\": \"Absolutely phenomenal! The chef's special was incredible and our server was attentive without being intrusive. Highly recommend!\", \"stars\": 5},\n",
    "        {\"text\": \"Perfect evening! Great food, wonderful ambiance, and exceptional service. This place exceeded all expectations!\", \"stars\": 5},\n",
    "        {\"text\": \"Fantastic restaurant! The menu is creative, portions are generous, and everything was delicious. Can't wait to return!\", \"stars\": 5},\n",
    "        \n",
    "        # 4-star reviews\n",
    "        {\"text\": \"Good food but the service was a bit slow. The restaurant was clean and the prices were reasonable.\", \"stars\": 4},\n",
    "        {\"text\": \"Pretty good overall. The pasta was delicious and the wine selection was impressive. Slightly expensive but worth it.\", \"stars\": 4},\n",
    "        {\"text\": \"Really enjoyed our meal. Food was tasty and well-presented. Only complaint is the wait time for our table.\", \"stars\": 4},\n",
    "        {\"text\": \"Nice restaurant with good food. Service was friendly and efficient. Would recommend for a casual dinner.\", \"stars\": 4},\n",
    "        {\"text\": \"Solid choice for dinner. The steak was cooked perfectly and the sides were great. Atmosphere could be better.\", \"stars\": 4},\n",
    "        \n",
    "        # 3-star reviews\n",
    "        {\"text\": \"Average experience. Food was okay, nothing special. Service was decent but could be better.\", \"stars\": 3},\n",
    "        {\"text\": \"It's an okay place. Food is decent, service is average. Nothing to complain about but nothing extraordinary either.\", \"stars\": 3},\n",
    "        {\"text\": \"Mixed experience. Some dishes were good, others were mediocre. Service was inconsistent throughout the evening.\", \"stars\": 3},\n",
    "        {\"text\": \"Decent restaurant. Food was acceptable and service was fine. Not bad but not great either.\", \"stars\": 3},\n",
    "        {\"text\": \"Average meal. The appetizer was good but the main course was disappointing. Service was okay.\", \"stars\": 3},\n",
    "        \n",
    "        # 2-star reviews\n",
    "        {\"text\": \"Disappointing meal. The food was cold when it arrived and the server seemed uninterested. Overpriced for what we got.\", \"stars\": 2},\n",
    "        {\"text\": \"Not impressed. The food took forever to arrive and when it did, it was lukewarm. The staff seemed overwhelmed.\", \"stars\": 2},\n",
    "        {\"text\": \"Below expectations. Food was bland and service was poor. The restaurant was also quite noisy.\", \"stars\": 2},\n",
    "        {\"text\": \"Disappointing experience. Long wait times, mediocre food, and inattentive service. Won't be returning.\", \"stars\": 2},\n",
    "        {\"text\": \"Not great. Food was underseasoned and service was slow. The place also felt dirty and unkempt.\", \"stars\": 2},\n",
    "        \n",
    "        # 1-star reviews\n",
    "        {\"text\": \"Terrible experience! Rude staff, awful food, and dirty restaurant. Would never recommend this place to anyone.\", \"stars\": 1},\n",
    "        {\"text\": \"Worst restaurant ever! Food was inedible, service was horrible, and the place was filthy. Complete waste of money!\", \"stars\": 1},\n",
    "        {\"text\": \"Absolutely awful! The food was disgusting, staff was rude, and the restaurant was dirty. Avoid at all costs!\", \"stars\": 1},\n",
    "        {\"text\": \"Horrible experience! Food poisoning from undercooked chicken, terrible service, and unsanitary conditions.\", \"stars\": 1},\n",
    "        {\"text\": \"Worst meal ever! Everything was wrong - cold food, rude waiters, dirty tables. Never coming back!\", \"stars\": 1}\n",
    "    ]\n",
    "    \n",
    "    # Repeat and shuffle to create larger dataset\n",
    "    extended_reviews = sample_reviews * (size // len(sample_reviews) + 1)\n",
    "    return pd.DataFrame(extended_reviews[:size])\n",
    "\n",
    "# Load data\n",
    "df = create_sample_data(50)  # Using 50 samples for demo\n",
    "\n",
    "print(f\"üìä Dataset loaded: {len(df)} reviews\")\n",
    "print(f\"üìà Rating distribution:\")\n",
    "print(df['stars'].value_counts().sort_index())\n",
    "\n",
    "# Display sample reviews\n",
    "print(\"\\nüìù Sample reviews:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}. Rating: {df.iloc[i]['stars']} stars\")\n",
    "    print(f\"   Review: {df.iloc[i]['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Direct Classification\n",
    "\n",
    "Simple, straightforward prompt asking for star rating classification with clear criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_1_direct_classification(review_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Approach 1: Direct Classification\n",
    "    Simple, straightforward prompt asking for star rating classification\n",
    "    \"\"\"\n",
    "    prompt = f'''\n",
    "    You are a review rating classifier. Analyze the following restaurant review and predict the star rating from 1 to 5 stars.\n",
    "    \n",
    "    Review: \"{review_text}\"\n",
    "    \n",
    "    Respond with a JSON object in this exact format:\n",
    "    {{\n",
    "        \"predicted_stars\": <number from 1-5>,\n",
    "        \"explanation\": \"<brief reasoning for the assigned rating>\"\n",
    "    }}\n",
    "    \n",
    "    Consider:\n",
    "    - 5 stars: Excellent, outstanding experience\n",
    "    - 4 stars: Good, above average with minor issues\n",
    "    - 3 stars: Average, okay experience\n",
    "    - 2 stars: Below average, several issues\n",
    "    - 1 star: Poor, terrible experience\n",
    "    '''\n",
    "    \n",
    "    return call_openai_api(prompt, \"Direct Classification\")\n",
    "\n",
    "print(\"‚úÖ Approach 1 (Direct Classification) defined\")\n",
    "print(\"üìã Strategy: Simple classification with clear rating criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Sentiment Analysis\n",
    "\n",
    "Focus on detailed sentiment analysis with aspect-based evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_2_sentiment_analysis(review_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Approach 2: Sentiment-Based Analysis\n",
    "    Focus on sentiment analysis with detailed reasoning\n",
    "    \"\"\"\n",
    "    prompt = f'''\n",
    "    As an expert sentiment analyst, evaluate this restaurant review by analyzing:\n",
    "    1. Overall sentiment (positive/negative/neutral)\n",
    "    2. Specific aspects mentioned (food, service, atmosphere, value)\n",
    "    3. Intensity of emotions expressed\n",
    "    4. Language tone and word choice\n",
    "    \n",
    "    Review: \"{review_text}\"\n",
    "    \n",
    "    Based on your analysis, assign a star rating (1-5) where:\n",
    "    - Very positive sentiment with praise = 4-5 stars\n",
    "    - Mostly positive with some concerns = 3-4 stars  \n",
    "    - Neutral or mixed sentiment = 2-3 stars\n",
    "    - Mostly negative sentiment = 1-2 stars\n",
    "    - Very negative with strong criticism = 1 star\n",
    "    \n",
    "    Return your response as JSON:\n",
    "    {{\n",
    "        \"predicted_stars\": <1-5>,\n",
    "        \"explanation\": \"<detailed reasoning based on sentiment analysis>\"\n",
    "    }}\n",
    "    '''\n",
    "    \n",
    "    return call_openai_api(prompt, \"Sentiment Analysis\")\n",
    "\n",
    "print(\"‚úÖ Approach 2 (Sentiment Analysis) defined\")\n",
    "print(\"üîç Strategy: Deep sentiment analysis with aspect-based evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Comparative Analysis\n",
    "\n",
    "Use few-shot learning with example reviews for consistent benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_3_comparative_analysis(review_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Approach 3: Comparative Analysis with Examples\n",
    "    Use few-shot learning with example reviews\n",
    "    \"\"\"\n",
    "    prompt = f'''\n",
    "    You are an experienced restaurant reviewer. Rate this review by comparing it to these examples:\n",
    "    \n",
    "    EXAMPLES:\n",
    "    5 Stars: \"Absolutely phenomenal! Best meal of my life. Perfect service, amazing atmosphere.\"\n",
    "    4 Stars: \"Really good food and service. Had a great time, just minor wait for table.\"\n",
    "    3 Stars: \"Decent place. Food was okay, service was fine. Nothing special but acceptable.\"\n",
    "    2 Stars: \"Food was cold, service was slow. Disappointed but not the worst experience.\"\n",
    "    1 Star: \"Terrible! Rude staff, awful food, dirty restaurant. Complete disaster.\"\n",
    "    \n",
    "    Now rate this review: \"{review_text}\"\n",
    "    \n",
    "    Compare the language, sentiment, and specific complaints/praise to the examples above.\n",
    "    \n",
    "    Provide your rating as JSON:\n",
    "    {{\n",
    "        \"predicted_stars\": <1-5>,\n",
    "        \"explanation\": \"<comparison-based reasoning>\"\n",
    "    }}\n",
    "    '''\n",
    "    \n",
    "    return call_openai_api(prompt, \"Comparative Analysis\")\n",
    "\n",
    "print(\"‚úÖ Approach 3 (Comparative Analysis) defined\")\n",
    "print(\"üìä Strategy: Few-shot learning with example-based comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Integration & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_api(prompt: str, approach: str) -> Dict:\n",
    "    \"\"\"Make API call to OpenAI with error handling and retry logic\"\"\"\n",
    "    max_retries = 3\n",
    "    retry_delay = 1\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that analyzes restaurant reviews and returns valid JSON responses.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=200,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            content = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Extract JSON from response\n",
    "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                result = json.loads(json_str)\n",
    "                \n",
    "                # Validate required fields\n",
    "                if 'predicted_stars' in result and 'explanation' in result:\n",
    "                    # Ensure stars is in valid range\n",
    "                    stars = int(result['predicted_stars'])\n",
    "                    if 1 <= stars <= 5:\n",
    "                        return {\n",
    "                            'predicted_stars': stars,\n",
    "                            'explanation': result['explanation'],\n",
    "                            'valid_json': True,\n",
    "                            'approach': approach\n",
    "                        }\n",
    "            \n",
    "            # If we get here, JSON was invalid\n",
    "            return {\n",
    "                'predicted_stars': 3,  # Default fallback\n",
    "                'explanation': 'Invalid JSON response from API',\n",
    "                'valid_json': False,\n",
    "                'approach': approach,\n",
    "                'raw_response': content\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"API call failed (attempt {attempt + 1}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2\n",
    "            else:\n",
    "                return {\n",
    "                    'predicted_stars': 3,\n",
    "                    'explanation': f'API call failed: {str(e)}',\n",
    "                    'valid_json': False,\n",
    "                    'approach': approach\n",
    "                }\n",
    "\n",
    "print(\"üîß API integration functions ready\")\n",
    "print(\"üõ°Ô∏è Error handling and retry logic implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework\n",
    "\n",
    "Comprehensive evaluation of all three approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_approach(df: pd.DataFrame, approach_func, approach_name: str) -> Dict:\n",
    "    \"\"\"Evaluate a single approach on the dataset\"\"\"\n",
    "    print(f\"\\nüîÑ Evaluating {approach_name}...\")\n",
    "    \n",
    "    predictions = []\n",
    "    actual_ratings = []\n",
    "    valid_json_count = 0\n",
    "    explanations = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"Processing review {idx + 1}/{len(df)}\", end='\\r')\n",
    "        \n",
    "        result = approach_func(row['text'])\n",
    "        predictions.append(result['predicted_stars'])\n",
    "        actual_ratings.append(row['stars'])\n",
    "        explanations.append(result['explanation'])\n",
    "        \n",
    "        if result['valid_json']:\n",
    "            valid_json_count += 1\n",
    "        \n",
    "        # Add small delay to respect API rate limits\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(actual_ratings, predictions)\n",
    "    json_validity_rate = valid_json_count / len(df)\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_report = classification_report(actual_ratings, predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    results = {\n",
    "        'approach_name': approach_name,\n",
    "        'accuracy': accuracy,\n",
    "        'json_validity_rate': json_validity_rate,\n",
    "        'predictions': predictions,\n",
    "        'actual_ratings': actual_ratings,\n",
    "        'explanations': explanations,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': confusion_matrix(actual_ratings, predictions)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ {approach_name} Results:\")\n",
    "    print(f\"   üìä Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"   üìã JSON Validity Rate: {json_validity_rate:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üìè Evaluation framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running All Evaluations\n",
    "\n",
    "Execute all three approaches and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all evaluations\n",
    "print(\"üöÄ Starting comprehensive evaluation...\")\n",
    "print(\"‚è±Ô∏è This may take a few minutes due to API calls...\")\n",
    "\n",
    "approaches = [\n",
    "    (approach_1_direct_classification, \"Direct Classification\"),\n",
    "    (approach_2_sentiment_analysis, \"Sentiment Analysis\"),\n",
    "    (approach_3_comparative_analysis, \"Comparative Analysis\")\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for approach_func, approach_name in approaches:\n",
    "    results = evaluate_approach(df, approach_func, approach_name)\n",
    "    all_results[approach_name] = results\n",
    "\n",
    "print(\"\\nüéâ All evaluations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "def create_comparison_table(results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Create comparison table of all approaches\"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for approach_name, result in results.items():\n",
    "        comparison_data.append({\n",
    "            'Approach': approach_name,\n",
    "            'Accuracy': f\"{result['accuracy']:.3f}\",\n",
    "            'JSON Validity Rate': f\"{result['json_validity_rate']:.3f}\",\n",
    "            'Precision (Macro Avg)': f\"{result['classification_report']['macro avg']['precision']:.3f}\",\n",
    "            'Recall (Macro Avg)': f\"{result['classification_report']['macro avg']['recall']:.3f}\",\n",
    "            'F1-Score (Macro Avg)': f\"{result['classification_report']['macro avg']['f1-score']:.3f}\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display comparison table\n",
    "comparison_df = create_comparison_table(all_results)\n",
    "print(\"üìä APPROACH COMPARISON TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best approach\n",
    "best_approach = max(all_results.keys(), key=lambda x: all_results[x]['accuracy'])\n",
    "print(f\"\\nüèÜ Best Performing Approach: {best_approach}\")\n",
    "print(f\"üìà Best Accuracy: {all_results[best_approach]['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Fynd AI Assessment - Rating Prediction Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "approaches = list(all_results.keys())\n",
    "accuracies = [all_results[app]['accuracy'] for app in approaches]\n",
    "\n",
    "bars1 = axes[0, 0].bar(approaches, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0, 0].set_title('Accuracy Comparison', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. JSON Validity Rate\n",
    "json_rates = [all_results[app]['json_validity_rate'] for app in approaches]\n",
    "bars2 = axes[0, 1].bar(approaches, json_rates, color=['#d62728', '#9467bd', '#8c564b'])\n",
    "axes[0, 1].set_title('JSON Validity Rate', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Validity Rate')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "for i, v in enumerate(json_rates):\n",
    "    axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Confusion Matrix for Best Approach\n",
    "cm = all_results[best_approach]['confusion_matrix']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
    "axes[1, 0].set_title(f'Confusion Matrix - {best_approach}', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "\n",
    "# 4. Rating Distribution Comparison\n",
    "for i, (approach, result) in enumerate(all_results.items()):\n",
    "    pred_dist = pd.Series(result['predictions']).value_counts().sort_index()\n",
    "    axes[1, 1].plot(pred_dist.index, pred_dist.values, marker='o', label=f'{approach} (Predicted)', linewidth=2)\n",
    "\n",
    "actual_dist = pd.Series(all_results[approaches[0]]['actual_ratings']).value_counts().sort_index()\n",
    "axes[1, 1].plot(actual_dist.index, actual_dist.values, marker='s', label='Actual', linewidth=3, color='black')\n",
    "axes[1, 1].set_title('Rating Distribution Comparison', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Star Rating')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Visualizations generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FYND AI ASSESSMENT - TASK 1 EVALUATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"   ‚Ä¢ Total Reviews Analyzed: {len(df)}\")\n",
    "print(f\"   ‚Ä¢ Rating Distribution: {dict(df['stars'].value_counts().sort_index())}\")\n",
    "\n",
    "print(f\"\\nüîç Detailed Analysis:\")\n",
    "for approach_name, result in all_results.items():\n",
    "    print(f\"\\n   {approach_name}:\")\n",
    "    print(f\"     ‚úì Accuracy: {result['accuracy']:.3f}\")\n",
    "    print(f\"     ‚úì JSON Validity: {result['json_validity_rate']:.3f}\")\n",
    "    print(f\"     ‚úì Precision: {result['classification_report']['macro avg']['precision']:.3f}\")\n",
    "    print(f\"     ‚úì Recall: {result['classification_report']['macro avg']['recall']:.3f}\")\n",
    "    print(f\"     ‚úì F1-Score: {result['classification_report']['macro avg']['f1-score']:.3f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Key Findings:\")\n",
    "print(f\"   ‚Ä¢ Best Approach: {best_approach}\")\n",
    "print(f\"   ‚Ä¢ Highest Accuracy: {all_results[best_approach]['accuracy']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Most Reliable JSON: {max(all_results.keys(), key=lambda x: all_results[x]['json_validity_rate'])}\")\n",
    "\n",
    "# Save results\n",
    "results_summary = {\n",
    "    'dataset_size': len(df),\n",
    "    'approaches': {\n",
    "        name: {\n",
    "            'accuracy': float(result['accuracy']),\n",
    "            'json_validity_rate': float(result['json_validity_rate']),\n",
    "            'precision': float(result['classification_report']['macro avg']['precision']),\n",
    "            'recall': float(result['classification_report']['macro avg']['recall']),\n",
    "            'f1_score': float(result['classification_report']['macro avg']['f1-score'])\n",
    "        }\n",
    "        for name, result in all_results.items()\n",
    "    },\n",
    "    'best_approach': best_approach\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Results saved to 'evaluation_results.json'\")\n",
    "print(\"üéØ Task 1 evaluation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions from each approach\n",
    "print(\"\\nüîç SAMPLE PREDICTIONS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(min(3, len(df))):\n",
    "    review = df.iloc[i]\n",
    "    print(f\"\\nüìù Review {i+1}:\")\n",
    "    print(f\"   Text: {review['text'][:100]}...\")\n",
    "    print(f\"   Actual Rating: {review['stars']} stars\")\n",
    "    print(f\"   Predictions:\")\n",
    "    \n",
    "    for approach_name, result in all_results.items():\n",
    "        predicted = result['predictions'][i]\n",
    "        explanation = result['explanations'][i][:80] + \"...\" if len(result['explanations'][i]) > 80 else result['explanations'][i]\n",
    "        accuracy_indicator = \"‚úÖ\" if predicted == review['stars'] else \"‚ùå\"\n",
    "        print(f\"     {accuracy_indicator} {approach_name}: {predicted} stars - {explanation}\")\n",
    "\n",
    "print(\"\\nüéâ Analysis complete! Check the generated visualizations and results above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}